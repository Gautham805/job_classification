{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# job_monitoring_system.py\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "from datetime import datetime\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# Constants\n",
        "BASE_URL = \"https://www.karkidi.com\"\n",
        "SEARCH_URL = \"https://www.karkidi.com/Find-Jobs/{page}/all/India?search={query}\"\n",
        "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "MODEL_DIR = \"model\"\n",
        "DATA_PATH = \"data/jobs.csv\"\n",
        "N_CLUSTERS = 5\n",
        "\n",
        "# Ensure directories exist\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "def log(msg):\n",
        "    print(f\"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] {msg}\")\n",
        "\n",
        "# Scrape job listings from multiple pages\n",
        "def scrape_jobs(keyword=\"data science\", pages=2):\n",
        "    jobs = []\n",
        "    query = keyword.replace(\" \", \"%20\")\n",
        "\n",
        "    for page in range(1, pages + 1):\n",
        "        url = SEARCH_URL.format(page=page, query=query)\n",
        "        log(f\"Scraping page {page}: {url}\")\n",
        "        try:\n",
        "            response = requests.get(url, headers=HEADERS, timeout=10)\n",
        "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "            job_blocks = soup.find_all(\"div\", class_=\"ads-details\")\n",
        "\n",
        "            for job in job_blocks:\n",
        "                try:\n",
        "                    title = job.find(\"h4\").get_text(strip=True)\n",
        "                    company = job.find(\"a\", href=lambda x: x and \"Employer-Profile\" in x)\n",
        "                    company_name = company.get_text(strip=True) if company else \"\"\n",
        "                    location = job.find(\"p\").get_text(strip=True) if job.find(\"p\") else \"\"\n",
        "                    experience = job.find(\"p\", class_=\"emp-exp\").get_text(strip=True) if job.find(\"p\", class_=\"emp-exp\") else \"\"\n",
        "                    skills_tag = job.find(\"span\", string=\"Key Skills\")\n",
        "                    skills = skills_tag.find_next(\"p\").get_text(strip=True) if skills_tag else \"\"\n",
        "                    summary_tag = job.find(\"span\", string=\"Summary\")\n",
        "                    summary = summary_tag.find_next(\"p\").get_text(strip=True) if summary_tag else \"\"\n",
        "                    url = BASE_URL + job.find_parent(\"a\")[\"href\"] if job.find_parent(\"a\") else \"\"\n",
        "\n",
        "                    jobs.append({\n",
        "                        \"title\": title,\n",
        "                        \"company\": company_name,\n",
        "                        \"location\": location,\n",
        "                        \"experience\": experience,\n",
        "                        \"skills\": skills,\n",
        "                        \"summary\": summary,\n",
        "                        \"url\": url,\n",
        "                        \"scraped_at\": datetime.now().isoformat()\n",
        "                    })\n",
        "                except Exception as e:\n",
        "                    log(f\"‚ùå Error parsing job block: {e}\")\n",
        "                    continue\n",
        "\n",
        "            time.sleep(1)\n",
        "        except Exception as e:\n",
        "            log(f\"‚ùå Failed to fetch page {page}: {e}\")\n",
        "\n",
        "    df = pd.DataFrame(jobs)\n",
        "    if 'skills' not in df.columns:\n",
        "        df['skills'] = \"none\"\n",
        "    return df\n",
        "\n",
        "# Preprocess skill strings\n",
        "def preprocess_skills(skills_series):\n",
        "    return skills_series.str.lower().str.replace(r\"[^a-zA-Z0-9, ]\", \"\", regex=True).fillna(\"\")\n",
        "\n",
        "# Train clustering model\n",
        "def train_model(jobs_df):\n",
        "    skills_cleaned = preprocess_skills(jobs_df['skills'])\n",
        "    vectorizer = TfidfVectorizer(tokenizer=str.split, stop_words=\"english\")\n",
        "    skill_vectors = vectorizer.fit_transform(skills_cleaned)\n",
        "\n",
        "    model = KMeans(n_clusters=N_CLUSTERS, random_state=42)\n",
        "    model.fit(skill_vectors)\n",
        "\n",
        "    jobs_df['cluster'] = model.labels_\n",
        "\n",
        "    # Evaluate with silhouette score\n",
        "    score = silhouette_score(skill_vectors, model.labels_)\n",
        "    log(f\"üìä Silhouette Score: {score:.3f}\")\n",
        "\n",
        "    # Manual inspection of clusters\n",
        "    for i in range(N_CLUSTERS):\n",
        "        log(f\"\\nüìÇ Cluster {i} sample skills:\")\n",
        "        print(jobs_df[jobs_df['cluster'] == i]['skills'].head(3).to_string(index=False))\n",
        "\n",
        "    joblib.dump(vectorizer, os.path.join(MODEL_DIR, \"skill_vectorizer.pkl\"))\n",
        "    joblib.dump(model, os.path.join(MODEL_DIR, \"clustering_model.pkl\"))\n",
        "\n",
        "    return jobs_df\n",
        "\n",
        "# Classify new jobs with saved model\n",
        "def classify_jobs(jobs_df):\n",
        "    vectorizer = joblib.load(os.path.join(MODEL_DIR, \"skill_vectorizer.pkl\"))\n",
        "    model = joblib.load(os.path.join(MODEL_DIR, \"clustering_model.pkl\"))\n",
        "\n",
        "    skills_cleaned = preprocess_skills(jobs_df['skills'])\n",
        "    skill_vectors = vectorizer.transform(skills_cleaned)\n",
        "    jobs_df['cluster'] = model.predict(skill_vectors)\n",
        "    return jobs_df\n",
        "\n",
        "# Save to CSV\n",
        "def save_jobs(jobs_df):\n",
        "    jobs_df.to_csv(DATA_PATH, index=False)\n",
        "    log(f\"‚úÖ Saved {len(jobs_df)} jobs to {DATA_PATH}\")\n",
        "\n",
        "# Notify users\n",
        "def notify_users(jobs_df, user_interest_clusters):\n",
        "    matching_jobs = jobs_df[jobs_df['cluster'].isin(user_interest_clusters)]\n",
        "    if not matching_jobs.empty:\n",
        "        log(\"üîî New jobs matching user interests:\")\n",
        "        print(matching_jobs[['title', 'company', 'location', 'url']])\n",
        "    else:\n",
        "        log(\"No matching jobs found today.\")\n",
        "\n",
        "# Main daily job\n",
        "if __name__ == \"__main__\":\n",
        "    log(\"üöÄ Starting job scraping and classification...\")\n",
        "    jobs_df = scrape_jobs(keyword=\"data science\", pages=2)\n",
        "\n",
        "    if not os.path.exists(os.path.join(MODEL_DIR, \"clustering_model.pkl\")):\n",
        "        log(\"üîß Training new clustering model...\")\n",
        "        jobs_df = train_model(jobs_df)\n",
        "    else:\n",
        "        log(\"üîé Classifying with existing model...\")\n",
        "        jobs_df = classify_jobs(jobs_df)\n",
        "\n",
        "    save_jobs(jobs_df)\n",
        "\n",
        "    # Example user preferences\n",
        "    user_interest_clusters = [0, 2]\n",
        "    notify_users(jobs_df, user_interest_clusters)\n",
        "\n",
        "    log(\"‚úÖ Job monitoring completed.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvOl9W9UVsUa",
        "outputId": "e2639d5d-1904-43cb-a3fd-7fc92be64ad7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-05-20 06:29:05] üöÄ Starting job scraping and classification...\n",
            "[2025-05-20 06:29:05] Scraping page 1: https://www.karkidi.com/Find-Jobs/1/all/India?search=data%20science\n",
            "[2025-05-20 06:29:17] Scraping page 2: https://www.karkidi.com/Find-Jobs/2/all/India?search=data%20science\n",
            "[2025-05-20 06:29:25] üîé Classifying with existing model...\n",
            "[2025-05-20 06:29:25] ‚úÖ Saved 20 jobs to data/jobs.csv\n",
            "[2025-05-20 06:29:25] üîî New jobs matching user interests:\n",
            "                                        title         company  \\\n",
            "4   Applied AI ML Director - Machine Learning  JPMorgan Chase   \n",
            "5                     Senior Product Designer      Observe.AI   \n",
            "7                              Data Scientist         Spotify   \n",
            "14  Applied AI ML Director - Machine Learning  JPMorgan Chase   \n",
            "15                    Senior Product Designer      Observe.AI   \n",
            "17                             Data Scientist         Spotify   \n",
            "\n",
            "                       location url  \n",
            "4   Hyderabad, Telangana, India      \n",
            "5   Bangalore, Karnataka, India      \n",
            "7    Mumbai, Maharashtra, India      \n",
            "14  Hyderabad, Telangana, India      \n",
            "15  Bangalore, Karnataka, India      \n",
            "17   Mumbai, Maharashtra, India      \n",
            "[2025-05-20 06:29:25] ‚úÖ Job monitoring completed.\n"
          ]
        }
      ]
    }
  ]
}